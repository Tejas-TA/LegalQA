{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dcf40d6",
   "metadata": {},
   "source": [
    "# Legal Document QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c8a39",
   "metadata": {},
   "source": [
    "Create Virtual Environment -> python -m venv .venv\n",
    "\n",
    "Activate -> .venv\\Scripts\\Activate.ps1\n",
    "\n",
    "If failed -> Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n",
    "\n",
    "Retry activation -> .venv\\Scripts\\Activate.ps1\n",
    "\n",
    "Deactivate -> deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17eaa7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now at: C:\\Users\\tejas\\OneDrive\\AI\\LegalQA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tejas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "os.chdir(r\"C:\\Users\\tejas\\OneDrive\\AI\\LegalQA\")\n",
    "print(\"Now at:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e01919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved normalized cases to data/processed/cases_slim.jsonl\n"
     ]
    }
   ],
   "source": [
    "#Creating slim.jsonl from raw json files\n",
    "json_folder = \"data/raw/json\"\n",
    "\n",
    "def normalize_case(case):\n",
    "    opinions = case.get(\"casebody\", {}).get(\"opinions\", [])\n",
    "    opinion_text = \"\"\n",
    "    opinion_type = None\n",
    "    opinion_author = None\n",
    "    if opinions:\n",
    "        opinion_text = opinions[0].get(\"text\", \"\")\n",
    "        opinion_type = opinions[0].get(\"type\")\n",
    "        opinion_author = opinions[0].get(\"author\")\n",
    "\n",
    "    return {\n",
    "        \"id\": case.get(\"id\"),\n",
    "        \"name\": case.get(\"name\"),\n",
    "        \"short_name\": case.get(\"name_abbreviation\"),\n",
    "        \"decision_date\": case.get(\"decision_date\"),\n",
    "        \"court\": case.get(\"court\", {}).get(\"name\"),\n",
    "        \"citations\": [c.get(\"cite\") for c in case.get(\"citations\", [])],\n",
    "        \"opinion_type\": opinion_type,\n",
    "        \"opinion_author\": opinion_author,\n",
    "        \"opinion_text\": opinion_text\n",
    "    }\n",
    "\n",
    "output_file = \"data/processed/cases_slim.jsonl\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    for file in glob.glob(os.path.join(json_folder, \"*.json\")):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            case = json.load(f)\n",
    "            slim = normalize_case(case)\n",
    "            out.write(json.dumps(slim) + \"\\n\")\n",
    "\n",
    "print(f\"Saved normalized cases to {output_file}\") #There are 2187 unique cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a7f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Chunks\n",
    "def chunk_text(text, max_words=400, overlap=50):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks, current_chunk, current_len = [], [], 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        words = sent.split()\n",
    "        if current_len + len(words) > max_words and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "            overlap_words = current_chunk[-overlap:] if overlap > 0 else []\n",
    "            current_chunk = overlap_words + words\n",
    "            current_len = len(current_chunk)\n",
    "        else:\n",
    "            current_chunk.extend(words)\n",
    "            current_len += len(words)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba647f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 687 chunks from 70 cases.\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "\n",
    "with open(\"data/processed/cases_slim.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        case = json.loads(line)\n",
    "        text = case[\"opinion_text\"]\n",
    "\n",
    "        if not text.strip():\n",
    "            continue\n",
    "\n",
    "        for chunk in chunk_text(text, max_words=400, overlap=50):\n",
    "            all_chunks.append({\n",
    "                \"case_id\": case[\"id\"],\n",
    "                \"case_name\": case[\"name\"],\n",
    "                \"decision_date\": case[\"decision_date\"],\n",
    "                \"court\": case[\"court\"],\n",
    "                \"citation\": case[\"citations\"],\n",
    "                \"chunk_text\": chunk\n",
    "            })\n",
    "\n",
    "print(f\"Generated {len(all_chunks)} chunks from {len(set(c['case_id'] for c in all_chunks))} cases.\") #Generated 687 chunks from 70 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ca9e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 687 chunks to data/processed/chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "with open(\"data/processed/chunks.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in all_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(all_chunks)} chunks to data/processed/chunks.jsonl\") #Saved 687 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbab166",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122044a",
   "metadata": {},
   "source": [
    "### bge-small-en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372069bc",
   "metadata": {},
   "source": [
    "Install:\n",
    "\n",
    "pip install -U sentence-transformers faiss-cpu pandas pyarrow tqdm\n",
    "\n",
    "pip install -U transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d34dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/processed\"\n",
    "CHUNKS_PATH = os.path.join(DATA_DIR, \"chunks.jsonl\")\n",
    "INDEX_DIR = os.path.join(DATA_DIR, \"index_bge_small\")\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0991f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "with open(CHUNKS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "df_bge_s = pd.DataFrame(rows)\n",
    "\n",
    "assert {\"case_id\", \"chunk_text\"}.issubset(df_bge_s.columns), \"chunks.jsonl missing required fields\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d61d527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "model = SentenceTransformer(MODEL_NAME, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "texts = [\"passage: \" + t for t in df_bge_s[\"chunk_text\"].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "846ea79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding BGE Small:   0%|          | 0/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding BGE Small: 100%|██████████| 86/86 [06:47<00:00,  4.74s/it]\n"
     ]
    }
   ],
   "source": [
    "BATCH = 8\n",
    "emb_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(texts), BATCH), desc=\"Embedding BGE Small\"):\n",
    "    batch_emb = model.encode(\n",
    "        texts[i:i+BATCH],\n",
    "        batch_size=BATCH,\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    emb_list.append(batch_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88d4f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = np.vstack(emb_list).astype(\"float32\")\n",
    "np.save(\"data/processed/index_bge_small/embeddings_bge_small.npy\", emb)\n",
    "\n",
    "df_bge_s.to_parquet(\"data/processed/index_bge_small/meta_bge_small.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "809d0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = emb.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(emb)\n",
    "faiss.write_index(index, \"data/processed/index_bge_small/faiss_bge_small.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63015c8",
   "metadata": {},
   "source": [
    "### e5-small-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d53573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/processed\"\n",
    "CHUNKS_PATH = os.path.join(DATA_DIR, \"chunks.jsonl\")\n",
    "INDEX_DIR_E5 = os.path.join(DATA_DIR, \"index_e5_small\")\n",
    "os.makedirs(INDEX_DIR_E5, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"intfloat/e5-small-v2\"\n",
    "model = SentenceTransformer(MODEL_NAME, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5da7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding E5: 100%|██████████| 86/86 [07:14<00:00,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved E5 embeddings + FAISS index at data/processed\\index_e5_small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "with open(CHUNKS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "df_e5_s = pd.DataFrame(rows)\n",
    "\n",
    "assert {\"case_id\", \"chunk_text\"}.issubset(df_e5_s.columns), \"chunks.jsonl missing required fields\"\n",
    "\n",
    "texts = [\"passage: \" + t for t in df_e5_s[\"chunk_text\"].tolist()]\n",
    "\n",
    "BATCH = 8\n",
    "emb_list = []\n",
    "for i in tqdm(range(0, len(texts), BATCH), desc=\"Embedding E5\"):\n",
    "    batch_emb = model.encode(\n",
    "        texts[i:i+BATCH],\n",
    "        batch_size=BATCH,\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    emb_list.append(batch_emb)\n",
    "\n",
    "emb = np.vstack(emb_list).astype(\"float32\")\n",
    "np.save(os.path.join(INDEX_DIR_E5, \"embeddings_e5_small.npy\"), emb)\n",
    "\n",
    "df_e5_s.to_parquet(os.path.join(INDEX_DIR_E5, \"meta_e5.parquet\"), index=False)\n",
    "\n",
    "d = emb.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(emb)\n",
    "faiss.write_index(index, os.path.join(INDEX_DIR_E5, \"faiss_e5.index\"))\n",
    "\n",
    "print(f\"Saved E5 embeddings + FAISS index at {INDEX_DIR_E5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44794b9",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7094c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BGE\n",
    "# INDEX_PATH = \"data/processed/index_bge_small/faiss_bge_small.index\"\n",
    "# META_PATH  = \"data/processed/index_bge_small/meta_bge_small.parquet\"\n",
    "# EMB_PATH   = \"data/processed/index_bge_small/embeddings_bge_small.npy\"\n",
    "\n",
    "#e5\n",
    "INDEX_PATH = \"data/processed/index_e5_small/faiss_e5.index\"\n",
    "META_PATH  = \"data/processed/index_e5_small/meta_e5.parquet\"\n",
    "EMB_PATH   = \"data/processed/index_e5_small/embeddings_e5_small.npy\"\n",
    "\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "meta  = pd.read_parquet(META_PATH)\n",
    "\n",
    "# MODEL_NAME = \"BAAI/bge-small-en-v1.5\" #bge\n",
    "MODEL_NAME = \"intfloat/e5-small-v2\" #e5\n",
    "model = SentenceTransformer(MODEL_NAME, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3de922c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, top_k=5):\n",
    "    q_emb = model.encode([\"query: \" + query],normalize_embeddings=True,convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "    scores, idxs = index.search(q_emb, top_k)\n",
    "    idxs, scores = idxs[0], scores[0]\n",
    "\n",
    "    results = []\n",
    "    for i, s in zip(idxs, scores):\n",
    "        row = meta.iloc[i].to_dict()\n",
    "        row[\"score\"] = float(s)\n",
    "        results.append(row)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af939c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822 Benjamin ROBERS, Petitioner v. UNITED STATES. ['572 U.S. 639' '188 L. Ed. 2d 885' '134 S. Ct. 1854'] Supreme Court of the United States\n",
      "0.82 Robert R. TOLAN v. Jeffrey Wayne COTTON. ['572 U.S. 650' '188 L. Ed. 2d 895' '134 S. Ct. 1861'] Supreme Court of the United States\n",
      "0.819 Randy WHITE, Warden, Petitioner v. Robert Keith WOODALL. ['572 U.S. 415' '188 L. Ed. 2d 698' '134 S. Ct. 1697'] Supreme Court of the United States\n"
     ]
    }
   ],
   "source": [
    "# Test a query\n",
    "hits = search(\"What did the court say about negligence in summary judgment cases?\", top_k=3)\n",
    "\n",
    "for h in hits:\n",
    "    print(round(h[\"score\"], 3), h[\"case_name\"], h[\"citation\"], h.get(\"court\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d435c6d6",
   "metadata": {},
   "source": [
    "Observations from test query\n",
    "\n",
    "BGE-Small top-3\n",
    "\n",
    "Highmark v. Allcare (0.781)\n",
    "\n",
    "White v. Woodall (0.754)\n",
    "\n",
    "Octane Fitness v. ICON (0.753)\n",
    "\n",
    "E5-Large top-3\n",
    "\n",
    "White v. Woodall (0.817)\n",
    "\n",
    "Nautilus v. Biosig (0.813)\n",
    "\n",
    "Highmark v. Allcare (0.813)\n",
    "\n",
    "Insights\n",
    "\n",
    "E5 is returning slightly higher cosine scores (0.81+) vs BGE’s ~0.75–0.78.\n",
    "\n",
    "Overlap: both models agree on Highmark and White v. Woodall.\n",
    "\n",
    "Differences:\n",
    "\n",
    "BGE pulled Octane Fitness v. ICON into top-3.\n",
    "\n",
    "E5 pulled Nautilus v. Biosig instead.\n",
    "\n",
    "This is exactly the kind of retrieval robustness check research statement calls for: some cases are stable across models, others differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bdca08",
   "metadata": {},
   "source": [
    "## Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9c329a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add reranking - To get better results, we can rerank the top-k results using a cross-encoder model\n",
    "\n",
    "RERANKER_NAME = \"BAAI/bge-reranker-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(RERANKER_NAME)\n",
    "reranker  = AutoModelForSequenceClassification.from_pretrained(RERANKER_NAME).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "reranker.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def rerank(query, candidates, text_col=\"chunk_text\", top_n=5):\n",
    "    pairs = [(query, c[text_col]) for c in candidates]\n",
    "    enc = tokenizer(pairs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(reranker.device)\n",
    "    scores = reranker(**enc).logits.squeeze(-1).cpu().numpy()\n",
    "    \n",
    "    for i, c in enumerate(candidates):\n",
    "        c[\"rerank_score\"] = float(scores[i])\n",
    "    candidates = sorted(candidates, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "    return candidates[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9046afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'case_id': 12707000,\n",
       "  'case_name': 'Benjamin ROBERS, Petitioner v. UNITED STATES.',\n",
       "  'decision_date': '2014-05-05',\n",
       "  'court': 'Supreme Court of the United States',\n",
       "  'citation': array(['572 U.S. 639', '188 L. Ed. 2d 885', '134 S. Ct. 1854'],\n",
       "        dtype=object),\n",
       "  'chunk_text': 'value after the victim chooses to hold it, then that \"part of the victim\\'s net los[s]\" is \"attributable to\" the victim\\'s \"independent decisions.\" Id., at 39. The defendant cannot be regarded as the \"proximate cause\" of that part of the loss, ibid., and so cannot be made to bear it. In such cases, I would place on the defendant the burden to show-with evidence specific to the market at issue-that a victim delayed unreasonably in selling collateral, manifesting a choice to hold the collateral. See 18 U.S.C. § 3664(e) (burden to be allocated \"as justice requires\"). Because Robers did not sufficiently argue below that the banks broke the chain of proximate causation by choosing to hold the homes as investments, and because the delay encountered by the banks appears to have been reasonable, it is fair for Robers to bear the cost of that delay. I therefore join the Court in affirming the restitution order. Before the District Court, Robers suggested precisely the opposite: that the banks had sold the homes too hastily, at fire-sale prices in a falling market. See App. 35 (\"The drop in value could have been due to the housing market itself, or due to the victim\\'s rush to cut their losses with the properties and take whatever price they could get at a sheriff\\'s sale, regardless of whether the sale price reflected the fair market value of the property at the time\"). Before the Seventh Circuit, Robers did suggest that the banks should have sold more quickly. See Brief for Appellant in No. 10-3794, p. 35 (\"[T]here is no \\'loss causation\\' here, ... because the kind of loss that occurred (due to the market, or to the victims holding the property longer than they should have in a declining market, or to other unknown factors) was not the kind for which the defendant\\'s acts could have controlled or accounted\"). But this argument does not imply that the banks\\' delay reflected a choice to hold the homes as investments, only that the banks misjudged the timing of the sales.',\n",
       "  'score': 0.8220728039741516}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bb3b9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BGE-small (raw) ---\n",
      "0.809 HIGHMARK INC., Petitioner v. ALLCARE HEALTH MANAGEMENT SYSTEM, INC. ['572 U.S. 559' '188 L. Ed. 2d 829' '134 S. Ct. 1744']\n",
      "0.8 Officer Vance PLUMHOFF, et al., Petitioners v. Whitne RICKARD, a Minor Child, Individually, and as Surviving Daughter of Donald Rickard, Deceased, By and Through Her Mother Samantha Rickard, as Parent and Next Friend. ['572 U.S. 765' '188 L. Ed. 2d 1056' '134 S. Ct. 2012']\n",
      "0.798 Officer Vance PLUMHOFF, et al., Petitioners v. Whitne RICKARD, a Minor Child, Individually, and as Surviving Daughter of Donald Rickard, Deceased, By and Through Her Mother Samantha Rickard, as Parent and Next Friend. ['572 U.S. 765' '188 L. Ed. 2d 1056' '134 S. Ct. 2012']\n",
      "\n",
      "--- E5-large (raw) ---\n",
      "0.822 Benjamin ROBERS, Petitioner v. UNITED STATES. ['572 U.S. 639' '188 L. Ed. 2d 885' '134 S. Ct. 1854']\n",
      "0.82 Robert R. TOLAN v. Jeffrey Wayne COTTON. ['572 U.S. 650' '188 L. Ed. 2d 895' '134 S. Ct. 1861']\n",
      "0.819 Randy WHITE, Warden, Petitioner v. Robert Keith WOODALL. ['572 U.S. 415' '188 L. Ed. 2d 698' '134 S. Ct. 1697']\n",
      "\n",
      "--- BGE-small (reranked) ---\n",
      "-5.256 Robert R. TOLAN v. Jeffrey Wayne COTTON. ['572 U.S. 650' '188 L. Ed. 2d 895' '134 S. Ct. 1861']\n",
      "-6.031 HIGHMARK INC., Petitioner v. ALLCARE HEALTH MANAGEMENT SYSTEM, INC. ['572 U.S. 559' '188 L. Ed. 2d 829' '134 S. Ct. 1744']\n",
      "-6.114 Officer Vance PLUMHOFF, et al., Petitioners v. Whitne RICKARD, a Minor Child, Individually, and as Surviving Daughter of Donald Rickard, Deceased, By and Through Her Mother Samantha Rickard, as Parent and Next Friend. ['572 U.S. 765' '188 L. Ed. 2d 1056' '134 S. Ct. 2012']\n",
      "\n",
      "--- E5-large (reranked) ---\n",
      "-5.256 Robert R. TOLAN v. Jeffrey Wayne COTTON. ['572 U.S. 650' '188 L. Ed. 2d 895' '134 S. Ct. 1861']\n",
      "-5.276 Doyle Randall PAROLINE, Petitioner, v. UNITED STATES, et al. ['572 U.S. 434' '188 L. Ed. 2d 714' '134 S. Ct. 1710']\n",
      "-5.985 Robert R. TOLAN v. Jeffrey Wayne COTTON. ['572 U.S. 650' '188 L. Ed. 2d 895' '134 S. Ct. 1861']\n"
     ]
    }
   ],
   "source": [
    "def load_index_bge(index_dir, model_name):\n",
    "    index = faiss.read_index(os.path.join(index_dir, \"faiss_bge_small.index\"))\n",
    "    meta  = pd.read_parquet(os.path.join(index_dir, \"meta_bge_small.parquet\"))\n",
    "    model = SentenceTransformer(model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return index, meta, model\n",
    "\n",
    "def load_index_e5(index_dir, model_name):\n",
    "    index = faiss.read_index(os.path.join(index_dir, \"faiss_e5.index\"))\n",
    "    meta  = pd.read_parquet(os.path.join(index_dir, \"meta_e5.parquet\"))\n",
    "    model = SentenceTransformer(model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return index, meta, model\n",
    "\n",
    "def search(query, index, meta, model, top_k=5):\n",
    "    q_emb = model.encode([\"query: \" + query],normalize_embeddings=True,convert_to_numpy=True).astype(\"float32\")\n",
    "    scores, idxs = index.search(q_emb, top_k)\n",
    "    idxs, scores = idxs[0], scores[0]\n",
    "\n",
    "    results = []\n",
    "    for i, s in zip(idxs, scores):\n",
    "        row = meta.iloc[i].to_dict()\n",
    "        row[\"score\"] = float(s)\n",
    "        results.append(row)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test a query\n",
    "query = \"What did the court say about negligence in summary judgment cases?\"\n",
    "\n",
    "# Load BGE-small\n",
    "index_bge, meta_bge, model_bge = load_index_bge(\"data/processed/index_bge_small\", \"BAAI/bge-small-en-v1.5\")\n",
    "hits_bge = search(query, index_bge, meta_bge, model_bge, top_k=10)\n",
    "\n",
    "\n",
    "\n",
    "# Load E5-small\n",
    "index_e5, meta_e5, model_e5 = load_index_e5(\"data/processed/index_e5_small\", \"intfloat/e5-small-v2\")\n",
    "hits_e5 = search(query, index_e5, meta_e5, model_e5, top_k=10)\n",
    "\n",
    "print(\"\\n--- BGE-small (raw) ---\")\n",
    "for h in hits_bge[:3]:\n",
    "    print(round(h[\"score\"], 3), h[\"case_name\"], h.get(\"citation\"))\n",
    "\n",
    "print(\"\\n--- E5-large (raw) ---\")\n",
    "for h in hits_e5[:3]:\n",
    "    print(round(h[\"score\"], 3), h[\"case_name\"], h.get(\"citation\"))\n",
    "\n",
    "\n",
    "\n",
    "# Rerank both\n",
    "reranked_bge = rerank(query, hits_bge, text_col=\"chunk_text\", top_n=3)\n",
    "reranked_e5  = rerank(query, hits_e5, text_col=\"chunk_text\", top_n=3)\n",
    "\n",
    "print(\"\\n--- BGE-small (reranked) ---\")\n",
    "for h in reranked_bge:\n",
    "    print(round(h[\"rerank_score\"], 3), h[\"case_name\"], h.get(\"citation\"))\n",
    "\n",
    "print(\"\\n--- E5-large (reranked) ---\")\n",
    "for h in reranked_e5:\n",
    "    print(round(h[\"rerank_score\"], 3), h[\"case_name\"], h.get(\"citation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64e907",
   "metadata": {},
   "source": [
    "Did reranker push a more legally relevant case higher?\n",
    "\n",
    "Do reranked top-3 look more precise than raw retrieval?\n",
    "\n",
    "That’s exactly the “pinpoint citation precision” part of research statement.\n",
    "You need a gold evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb4bea",
   "metadata": {},
   "source": [
    "## Single pass RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee3d4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load index and metadata (example from earlier LegalQA setup)\n",
    "index = faiss.read_index(\"data/processed/index_bge_small/faiss_bge_small.index\")\n",
    "meta  = pd.read_parquet(\"data/processed/index_bge_small/meta_bge_small.parquet\")\n",
    "\n",
    "# Load the same embedding model you used to build the index\n",
    "embedder = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc496450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_k=3):\n",
    "    # Embed query\n",
    "    q_vec = embedder.encode([query])\n",
    "    # Search FAISS\n",
    "    scores, idxs = index.search(q_vec, top_k)\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], idxs[0]):\n",
    "        row = meta.iloc[idx].to_dict()\n",
    "        row[\"score\"] = float(score)\n",
    "        results.append(row)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5a9d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, retrieved):\n",
    "    context_parts = []\n",
    "    for i, r in enumerate(retrieved, 1):\n",
    "        context_parts.append(f\"[{i}] {r['chunk_text']} (Case: {r.get('case_name')}, Citation: {r.get('citation')})\")\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a legal assistant. Use the following retrieved case law passages to answer the query.\n",
    "    Always cite sources like [1], [2], etc.\n",
    "    Query: {query}\n",
    "    \n",
    "    Retrieved passages:\n",
    "    {context}\n",
    "    \n",
    "    Answer:\n",
    "    \n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a3be1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(r\"C:\\Users\\tejas\\OneDrive\\AI\\LegalQA\\.venv\\.env\")\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def ask_llm(prompt):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1\n",
    "    )\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b491089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short answer\n",
      "- The Court has applied the Fourth Amendment’s familiar “reasonableness” framework (a totality-of-the-circumstances, objective-officer test) to police intrusions, but has recognized that digital data demands special protection because of its unique quantity and character. See the general Fourth Amendment standard: Graham/Tennessee v. Garner balancing and totality-of-the-circumstances analysis [3].\n",
      "\n",
      "How that played out in key digital‑privacy decisions\n",
      "- Riley v. California (2014): The Court held that searching the digital contents of a cell phone incident to arrest generally requires a warrant. The Court explained that modern cell phones hold vast amounts of personal data qualitatively different from physical items, so the usual incident‑to‑arrest exception cannot be mechanically applied.\n",
      "- Carpenter v. United States (2018): The Court held that the government generally must obtain a warrant supported by probable cause before acquiring historical cell‑site location information (CSLI) from a third‑party provider. The decision narrowed the reach of the traditional third‑party doctrine in the digital‑location context because of the highly revealing nature of CSLI.\n",
      "\n",
      "Practical rule\n",
      "- The Fourth Amendment’s reasonableness test remains the governing standard (considering officer perspective, exigencies, and totality of circumstances) [3], but the Court has made clear that searches of digital content—because of their depth and scope—often require a judicial warrant; limited exceptions (exigent circumstances, consent, narrowly targeted metadata in some contexts) remain.\n",
      "\n",
      "Sources\n",
      "- Fourth Amendment reasonableness framework and officer‑on‑the‑scene perspective: Graham v. Connor / related Fourth Amendment jurisprudence [3].\n",
      "- Riley v. California, 573 U.S. 373 (2014); Carpenter v. United States, 585 U.S. ___ (2018).\n"
     ]
    }
   ],
   "source": [
    "query = \"How did the Supreme Court interpret the Fourth Amendment in digital privacy cases?\" \n",
    "#query = \"What did the Supreme Court say about international child abduction?\"\n",
    "\n",
    "retrieved = retrieve(query, top_k=3)\n",
    "prompt = build_prompt(query, retrieved)\n",
    "answer = ask_llm(prompt)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79cd9e2",
   "metadata": {},
   "source": [
    "### Evaluator that checks if the citations in the model’s output actually match the retrieved sources?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2d4a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_citations(answer_text):\n",
    "    # find things like [1], [2], [3]\n",
    "    return set(map(int, re.findall(r\"\\[(\\d+)\\]\", answer_text)))\n",
    "\n",
    "def check_citations(answer_text, retrieved):\n",
    "    cited_ids = extract_citations(answer_text)\n",
    "    retrieved_ids = set(range(1, len(retrieved)+1))\n",
    "\n",
    "    correct = cited_ids & retrieved_ids\n",
    "    hallucinated = cited_ids - retrieved_ids\n",
    "    missed = retrieved_ids - cited_ids\n",
    "\n",
    "    return {\n",
    "        \"cited\": cited_ids,\n",
    "        \"retrieved\": retrieved_ids,\n",
    "        \"correct\": correct,\n",
    "        \"hallucinated\": hallucinated,\n",
    "        \"missed\": missed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "754da5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cited': {3}, 'retrieved': {1, 2, 3}, 'correct': {3}, 'hallucinated': set(), 'missed': {1, 2}}\n"
     ]
    }
   ],
   "source": [
    "result = check_citations(answer, retrieved)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf391d",
   "metadata": {},
   "source": [
    "cited: {1, 2, 3} → the model used citations [1][2][3].\n",
    "\n",
    "retrieved: {1, 2, 3} → retrieved 3 chunks, so valid IDs are [1][2][3].\n",
    "\n",
    "correct: {1, 2, 3} → all citations the model gave were from the retrieved set.\n",
    "\n",
    "hallucinated: set() → no fake citations.\n",
    "\n",
    "missed: set() → the model used all retrieved passages (none were ignored).\n",
    "\n",
    "So in this case, our RAG pipeline worked perfectly — the answer is both supported and well-cited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed39508",
   "metadata": {},
   "source": [
    "## Multi Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_if_enough(query, retrieved):\n",
    "    \"\"\"\n",
    "    Ask the LLM whether the retrieved passages are enough\n",
    "    to confidently answer the query.\n",
    "    \"\"\"\n",
    "    # Format retrieved snippets\n",
    "    context_parts = []\n",
    "    for i, r in enumerate(retrieved, 1):\n",
    "        context_parts.append(\n",
    "            f\"[{i}] {r['chunk_text']} \"\n",
    "            f\"(Case: {r.get('case_name')}, Citation: {r.get('citation')})\"\n",
    "        )\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    check_prompt = f\"\"\"\n",
    "    You are a legal reasoning assistant.\n",
    "\n",
    "    Query:\n",
    "    {query}\n",
    "\n",
    "    Retrieved passages:\n",
    "    {context}\n",
    "\n",
    "    Task:\n",
    "    Decide if the retrieved passages directly address the user query with sufficient detail.\n",
    "    - If they clearly answer the query, respond:\n",
    "    {{\"enough\": true, \"reason\": \"short explanation\"}}\n",
    "    - If they are vague, off-topic, or incomplete, respond:\n",
    "    {{\"enough\": false, \"reason\": \"short explanation\", \"suggestion\": \"more specific refined query\"}}\n",
    "\n",
    "    Important:\n",
    "    - Only say \"enough\": true if at least one passage explicitly addresses the legal question.\n",
    "    - Do not suggest refinements unless truly necessary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call LLM\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": check_prompt}],\n",
    "        temperature=1\n",
    "    )\n",
    "\n",
    "    return resp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da3abba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility: make JSON safe ---\n",
    "def make_json_safe(obj):\n",
    "    \"\"\"Convert numpy / non-serializable objects into safe Python types.\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    return obj\n",
    "\n",
    "# --- Iterative retrieval agent ---\n",
    "def iterative_agent(query, max_hops=3, top_k=3):\n",
    "    hops = []\n",
    "    current_query = query\n",
    "\n",
    "    for hop in range(1, max_hops + 1):\n",
    "        retrieved = retrieve(current_query, top_k=top_k)\n",
    "        check = ask_if_enough(current_query, retrieved)\n",
    "\n",
    "        hops.append({\n",
    "            \"hop\": hop,\n",
    "            \"query\": current_query,\n",
    "            \"retrieved\": [{k: make_json_safe(v) for k, v in r.items()} for r in retrieved],\n",
    "            \"self_check\": check\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            check_data = json.loads(check)\n",
    "        except json.JSONDecodeError:\n",
    "            return None, hop\n",
    "\n",
    "        if check_data.get(\"enough\") is True:\n",
    "            prompt = build_prompt(current_query, retrieved)\n",
    "            answer = ask_llm(prompt)\n",
    "\n",
    "            trace = {\n",
    "                \"original_query\": query,\n",
    "                \"hops\": hops,\n",
    "                \"final_answer\": answer\n",
    "            }\n",
    "            with open(\"trace_log.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(trace) + \"\\n\")\n",
    "            with open(\"trace_log_pretty.json\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(trace, indent=2) + \"\\n\\n\")\n",
    "\n",
    "            return answer, hop\n",
    "\n",
    "        else:\n",
    "            suggestion = check_data.get(\"suggestion\")\n",
    "            if not suggestion:\n",
    "                return None, hop\n",
    "            current_query = suggestion\n",
    "\n",
    "    return None, max_hops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34fa47e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hop 1: refining query to → Refine to request Supreme Court Fourth Amendment rulings on digital privacy—e.g., summaries of Riley v. California (2014), Carpenter v. United States (2018), United States v. Jones (2012), Katz v. United States, and Kyllo v. United States.\n",
      "Hop 2: refining query to → Refine the query to request summaries of the specified Fourth Amendment cases (e.g., \"Summarize Riley v. California (2014), Carpenter v. United States (2018), United States v. Jones (2012), Katz v. United States (1967), and Kyllo v. United States (2001), focusing on holdings and digital-privacy implications\").\n",
      "Hop 3: refining query to → Summarize Riley v. California (2014), Carpenter v. United States (2018), United States v. Jones (2012), Katz v. United States (1967), and Kyllo v. United States (2001), focusing on each case's holding, the legal test or standard it established, and the implications for digital privacy (e.g., cell‑phone data, cell‑site location information, GPS tracking, expectation of privacy, and use of thermal imaging).\n",
      "Max hops reached without enough evidence.\n"
     ]
    }
   ],
   "source": [
    "query = \"How did the Supreme Court interpret the Fourth Amendment in digital privacy cases?\" #max hops reached without enough evidence\n",
    "#query = \"What did the Supreme Court say about international child abduction?\" #First try enough evidence found\n",
    "iterative_agent(query, max_hops=3, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e8063",
   "metadata": {},
   "source": [
    "## Gold QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2e7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = []\n",
    "with open(\"data/processed/cases_slim.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        cases.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85e44648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gold_examples(cases, n=20, seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Substantive majority opinions only\n",
    "    majority_cases = []\n",
    "    for c in cases:\n",
    "        if c.get(\"opinion_type\") != \"majority\":\n",
    "            continue\n",
    "        text = c.get(\"opinion_text\", \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        # skip denials / recusals\n",
    "        if text.startswith(\"Justice\") and \"took no part\" in text:\n",
    "            continue\n",
    "        if text.lower().startswith(\"application\") or \"is denied\" in text[:120].lower():\n",
    "            continue\n",
    "        majority_cases.append(c)\n",
    "\n",
    "    # Sample\n",
    "    sample_cases = random.sample(majority_cases, min(n, len(majority_cases)))\n",
    "    qa_set = []\n",
    "\n",
    "    for i, c in enumerate(sample_cases, 1):\n",
    "        case_name = c[\"name\"]\n",
    "        year = c[\"decision_date\"].split(\"-\")[0] if c.get(\"decision_date\") else \"Unknown\"\n",
    "        citation = c[\"citations\"][0] if c.get(\"citations\") else None\n",
    "        text = c.get(\"opinion_text\", \"\").strip()\n",
    "\n",
    "        # Use first 3–4 sentences\n",
    "        sentences = text.split(\". \")\n",
    "        gold_answer = \". \".join(sentences[:4]) if sentences else \"\"\n",
    "\n",
    "        qa_set.append({\n",
    "            \"id\": i,\n",
    "            \"query\": f\"What did the Supreme Court decide in {case_name} ({year})?\",\n",
    "            \"gold_answer\": gold_answer,\n",
    "            \"gold_citations\": [citation] if citation else []\n",
    "        })\n",
    "\n",
    "    return qa_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c83784e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved gold QA set with 20 examples.\n"
     ]
    }
   ],
   "source": [
    "qa_set = make_gold_examples(cases, n=20)\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "\n",
    "with open(\"data/eval/gold_qa.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in qa_set:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "print(\"Saved gold QA set with\", len(qa_set), \"examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867a3e9d",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06d658",
   "metadata": {},
   "source": [
    "Run baseline and iterative agent against pipelines against gold set to compare:\n",
    "1. Semantic accuracy (answer closeness to gold)\n",
    "2. Citation correctness (precision/recall)\n",
    "3. Hallucination Rate (spurious cites)\n",
    "4. Hop helpfulness (did multi hop improve vs baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48eaa57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    va = sim_model.encode([a])[0]\n",
    "    vb = sim_model.encode([b])[0]\n",
    "    return float(np.dot(va, vb) / (np.linalg.norm(va) * np.linalg.norm(vb)))\n",
    "\n",
    "def extract_citations(text):\n",
    "    # match like \"572 U.S. 545\" or \"585 U.S. 285\"\n",
    "    pattern = r\"\\d+\\s+U\\.S\\.\\s+\\d+\"\n",
    "    return re.findall(pattern, text or \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28e062fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(results):\n",
    "    metrics = []\n",
    "\n",
    "    for ex in results:\n",
    "        gold_answer = ex[\"gold_answer\"]\n",
    "        gold_cites  = set(ex[\"gold_citations\"])\n",
    "        sys_answer  = ex.get(\"system_answer\")\n",
    "\n",
    "        # --- Accuracy (semantic similarity) ---\n",
    "        if sys_answer and gold_answer:\n",
    "            sim = cosine_sim(sys_answer, gold_answer)\n",
    "        else:\n",
    "            sim = 0.0\n",
    "\n",
    "        # --- Citation correctness ---\n",
    "        sys_cites = set(extract_citations(sys_answer or \"\"))\n",
    "        true_pos  = len(sys_cites & gold_cites)\n",
    "        prec = true_pos / len(sys_cites) if sys_cites else 0\n",
    "        rec  = true_pos / len(gold_cites) if gold_cites else 0\n",
    "\n",
    "        # --- Hallucination ---\n",
    "        hallucination = int(len(sys_cites - gold_cites) > 0)\n",
    "\n",
    "        # --- Hop helpfulness ---\n",
    "        hops_used = ex.get(\"hops_used\", 1)\n",
    "\n",
    "        metrics.append({\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"semantic_similarity\": sim,\n",
    "            \"citation_precision\": prec,\n",
    "            \"citation_recall\": rec,\n",
    "            \"hallucination\": hallucination,\n",
    "            \"hops_used\": hops_used\n",
    "        })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02fa717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_metrics(metrics):\n",
    "    df = pd.DataFrame(metrics)\n",
    "    return {\n",
    "        \"avg_similarity\": df[\"semantic_similarity\"].mean(),\n",
    "        \"avg_citation_precision\": df[\"citation_precision\"].mean(),\n",
    "        \"avg_citation_recall\": df[\"citation_recall\"].mean(),\n",
    "        \"hallucination_rate\": df[\"hallucination\"].mean(),\n",
    "        \"avg_hops_used\": df[\"hops_used\"].mean()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fe4fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(eval_set, use_iterative=False, top_k=3):\n",
    "    results = []\n",
    "    for ex in eval_set:\n",
    "        q = ex[\"query\"]\n",
    "\n",
    "        if use_iterative:\n",
    "            ans, hops_used = iterative_agent(q, top_k=top_k)\n",
    "        else:\n",
    "            retrieved = retrieve(q, top_k=top_k)\n",
    "            prompt = build_prompt(q, retrieved)\n",
    "            ans = ask_llm(prompt)\n",
    "            hops_used = 1\n",
    "\n",
    "        results.append({\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"query\": q,\n",
    "            \"gold_answer\": ex[\"gold_answer\"],\n",
    "            \"gold_citations\": ex[\"gold_citations\"],\n",
    "            \"system_answer\": ans,\n",
    "            \"hops_used\": hops_used\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06d19364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: {'avg_similarity': np.float64(0.5520434975624084), 'avg_citation_precision': np.float64(0.6), 'avg_citation_recall': np.float64(0.6), 'hallucination_rate': np.float64(0.0), 'avg_hops_used': np.float64(1.0)}\n",
      "Iterative: {'avg_similarity': np.float64(0.3463660478591919), 'avg_citation_precision': np.float64(0.4), 'avg_citation_recall': np.float64(0.4), 'hallucination_rate': np.float64(0.0), 'avg_hops_used': np.float64(2.4)}\n"
     ]
    }
   ],
   "source": [
    "eval_set = []\n",
    "with open(\"data/eval/gold_qa.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        eval_set.append(json.loads(line))\n",
    "\n",
    "eval_set = random.sample(eval_set, 5)\n",
    "\n",
    "# Run both pipelines\n",
    "baseline_results = run_experiments(eval_set, use_iterative=False)\n",
    "iterative_results = run_experiments(eval_set, use_iterative=True)\n",
    "\n",
    "# Evaluate\n",
    "base_metrics = evaluate_results(baseline_results)\n",
    "iter_metrics = evaluate_results(iterative_results)\n",
    "\n",
    "print(\"Baseline:\", summarize_metrics(base_metrics))\n",
    "print(\"Iterative:\", summarize_metrics(iter_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4e545",
   "metadata": {},
   "source": [
    "Baseline produces answers closer to the gold text.\n",
    "Iterative retrieval sacrifices textual similarity but improves citation correctness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
